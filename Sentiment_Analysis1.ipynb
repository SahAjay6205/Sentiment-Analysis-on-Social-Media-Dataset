{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d kazanova/sentiment140\n",
        "from zipfile import ZipFile\n",
        "dataset = '/content/sentiment140.zip'\n",
        "\n",
        "with ZipFile(dataset,'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('Done')\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import joblib\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "columns = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n",
        "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin1', header=None, names=columns)\n",
        "\n",
        "print(df.head())\n",
        "# Clean text\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab') # Download the punkt_tab data package\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "df['cleaned_text'] = df['text'].apply(clean_text)\n",
        "print(df[['text', 'cleaned_text']].head())\n",
        "import matplotlib.pyplot as plt  # Import matplotlib.pyplot\n",
        "import seaborn as sns  # Import seaborn\n",
        "\n",
        "df['sentiment'] = df['target'].map({0: 0, 4: 1})\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='sentiment', data=df)\n",
        "plt.title(\"Distribution of Encoded Sentiment Labels\")\n",
        "plt.show()\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "# LabelEncoder is typically used for categorical data with more than two classes.\n",
        "# Since we are already handling it above with mapping for binary classes it might not be needed\n",
        "# Using labelEncoder just for demonstration here.\n",
        "\n",
        "#df['sentiment'] = label_encoder.fit_transform(df['sentiment'])  # Now, this should work\n",
        "import matplotlib.pyplot as plt  # Import matplotlib.pyplot\n",
        "import seaborn as sns  # Import seaborn\n",
        "\n",
        "df['sentiment'] = df['target'].map({0: 0, 4: 1})\n",
        "\n",
        "# Visualization\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(x='sentiment', data=df)\n",
        "plt.title(\"Distribution of Encoded Sentiment Labels\")\n",
        "plt.show()\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "# LabelEncoder is typically used for categorical data with more than two classes.\n",
        "# Since we are already handling it above with mapping for binary classes it might not be needed\n",
        "# Using labelEncoder just for demonstration here.\n",
        "\n",
        "#df['sentiment'] = label_encoder.fit_transform(df['sentiment'])  # Now, this should work\n",
        "# Step 5: Vectorize text\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Visualization: Shape of vectorized data\n",
        "print(\"\\nShape of Vectorized Data:\")\n",
        "print(f\"X_train_vec shape: {X_train_vec.shape}\")\n",
        "print(f\"X_test_vec shape: {X_test_vec.shape}\")\n",
        "# Step 6: Train model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_vec, y_train)\n",
        "# Step 7: Evaluate model\n",
        "y_pred = model.predict(X_test_vec)\n",
        "# Step 8: Save model\n",
        "joblib.dump(model, 'sentiment_model.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')\n",
        "\n",
        "print(\"\\nModel and vectorizer saved successfully!\")\n",
        "\n",
        "# Visualization: Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(6, 4))\n",
        "# Since you're dealing with binary classification (0 and 1), manually set the labels:\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=[0, 1], yticklabels=[0, 1])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n",
        "\n",
        "# Visualization: Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['0', '1'])) # Manually provide target names\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "files.download('sentiment_model.pkl')\n",
        "files.download('tfidf_vectorizer.pkl')"
      ],
      "metadata": {
        "id": "jPTWZvx7Z4ME"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}